{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## This notebook is part of the Apache Spark training delivered by CERN-IT\n",
    "### Demo of Spark instrumenation on CERN SWAN\n",
    "Contact: Luca.Canali@cern.ch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run this notebook from Jupyter with Python kernel\n",
    "- When using on CERN SWAN, do not attach the notebook to a Spark cluster, but rather run locally on the SWAN container\n",
    "- If running this outside CERN SWAN, plese make sure to have PySpark installed: `pip install pyspark`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### First let's create a Spark Session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Do not run this cell when running on CERN SWAN\n",
    "# Rather use the Spark connector (the \"star\" button)\n",
    "#\n",
    "# When not running this on SWAN you also need additional steps\n",
    "# to see Spark metrics in the Spark dashboard ,\n",
    "# see https://github.com/cerndb/spark-dashboard\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder \\\n",
    "        .appName(\"my demo app\")  \\\n",
    "        .master(\"yarn\") \\\n",
    "        .config(\"spark.executor.memory\",\"4g\") \\\n",
    "        .config(\"spark.executor.cores\",\"4\") \\\n",
    "        .config(\"spark.jars.packages\",\"ch.cern.sparkmeasure:spark-measure_2.12:0.22\") \\\n",
    "        .config(\"spark.ui.showConsoleProgress\", \"false\") \\\n",
    "        .getOrCreate()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - in-memory</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://swan005.cern.ch:5192\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.3.1</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>yarn</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>pyspark_shell_swan</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x7f18f13008e0>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run a TPCDS benchmark query\n",
    "We use this to create some load, which allows to show the available Spark monitoring tools:\n",
    "spark monitor, the Spark Web UI, Spark dashboard integration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating temporary view date_dim\n",
      "22/11/02 16:49:13 WARN package: Truncated the string representation of a plan since it was too large. This behavior can be adjusted by setting 'spark.sql.debug.maxToStringFields'.\n",
      "Creating temporary view store_sales\n",
      "Creating temporary view item\n"
     ]
    }
   ],
   "source": [
    "# This uses TPCDS data in CERN Hadoop cluter\n",
    "path=\"hdfs://analytix/project/spark/TPCDS/tpcds_10000_parquet_1.12.2/\"\n",
    "tables = [\"date_dim\", \"store_sales\", \"item\"]\n",
    "\n",
    "for t in tables:\n",
    "  print(f\"Creating temporary view {t}\")\n",
    "  spark.read.parquet(path + t).createOrReplaceTempView(t)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## TPCDS benchmark query Q3\n",
    "q3 = \"\"\"\n",
    "SELECT dt.d_year, item.i_brand_id brand_id, item.i_brand brand,SUM(ss_ext_sales_price) sum_agg\n",
    "FROM  date_dim dt, store_sales, item\n",
    "WHERE dt.d_date_sk = store_sales.ss_sold_date_sk\n",
    "  AND store_sales.ss_item_sk = item.i_item_sk\n",
    "  AND item.i_manufact_id = 128\n",
    "  AND dt.d_moy=11\n",
    "GROUP BY dt.d_year, item.i_brand, item.i_brand_id\n",
    "ORDER BY dt.d_year, sum_agg desc, brand_id\n",
    "LIMIT 100\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run TPCDS query Q3\n",
    "result = spark.sql(q3).collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Demo:  \n",
    "   - while the query runs go to the Spark Web UI  \n",
    "   - also see Spark metrics visualized in the Spark dashboard  \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Additional tools: sparkMeasure\n",
    "SparkMeasure is an external tools that simplifies the collection and analysis of Spark performance metrics.   \n",
    "See: https://github.com/LucaCanali/sparkMeasure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# install the Python wrapper API\n",
    "# Note to use sparkMeasure you need configuration to get and use the jar, example\n",
    "# spark.jars.packages=ch.cern.sparkmeasure:spark-measure_2.12:0.22\n",
    "\n",
    "!pip install --user sparkmeasure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sparkmeasure import StageMetrics\n",
    "stagemetrics = StageMetrics(spark)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stagemetrics.runandmeasure(globals(), 'spark.sql(\"select count(*) from range(1000) cross join range(1000) cross join range(1000)\").show()')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stagemetrics.runandmeasure(globals(), f'spark.sql(\"\"\"{q3}\"\"\").collect()')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Increase Logging verbosity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# increase logging verbosity\n",
    "spark.sparkContext.setLogLevel(\"INFO\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run TPCDS query Q3\n",
    "result = spark.sql(q3).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  logging verbosity back to normal\n",
    "spark.sparkContext.setLogLevel(\"WARN\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "sparkconnect": {
   "bundled_options": [
    "SparkMetrics"
   ],
   "list_of_options": [
    {
     "name": "spark.jars.packages",
     "value": "ch.cern.sparkmeasure:spark-measure_2.12:0.22"
    }
   ]
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
